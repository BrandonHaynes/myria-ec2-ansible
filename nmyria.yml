---
- name: Create AWS resources
  hosts: localhost
  gather_facts: False
  vars:
    key_name: "{{ KEY_NAME|default(defaultkey) }}"
    instance_type: m1.small
    instance_count: 2
    security_group: default
    image: ami-6989a659
    region: us-west-2
    wait_timeout: 600
    new_pub_key: ~/.ssh/id_rsa_new.pub
    new_priv_key: ~/.ssh/id_rsa_new
  tasks:
  - name: Create security group  #so we can ssh into the instance from our laptop
    ec2_group:
      name: myriasec
      description: "A Security group"
      region: "{{region}}"
      rules:
      - proto: tcp
        type: ssh
        from_port: 22
        to_port: 22
        cidr_ip: 0.0.0.0/0
      - proto: tcp 
        type: myria
        from_port: 8001
        to_port: 9100
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        type: http
        from_port: 80
        to_port: 80
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        type: https
        from_port: 443
        to_port: 443
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        type: postgres
        from_port: 5432
        to_port: 5432
        cidr_ip: 0.0.0.0/0
      rules_egress:
      - proto: all
        type: all
        cidr_ip: 0.0.0.0/0
    register: basic_firewall

  - name: Launch instance
    ec2:
      keypair: "{{key_name}}"
      group_id: "{{basic_firewall.group_id}}"
      instance_type: "{{instance_type}}"
      image: "{{image}}"
      count: "{{instance_count}}"
      wait: true
      wait_timeout: "{{wait_timeout}}"
      region: "{{region}}"
      instance_tags:
        Name: "{{ lookup('env', 'USER') }}-{{ key_name }}"
    register: ec2

  - name: Add new instances to host group
    add_host: hostname={{item.public_ip}} groupname=nodes
    with_items: ec2.instances

  - name: Add first instance to coordinators group
    add_host: hostname={{item.public_ip}} groupname=coordinators
    with_items: ec2.instances[:1]

  - name: Add remaining instance to workers group
    add_host: hostname={{item.public_ip}} groupname=workers
    with_items: ec2.instances

  - add_host: hostname={{item.dns_name}} groupname=coordinator_name
    with_items: ec2.instances[:1]

  - add_host: hostname={{item.dns_name}} groupname=worker_names
    with_items: ec2.instances[1:]

  - name: Wait for SSH to come up
    local_action: wait_for host={{ item.public_dns_name }} port=22 delay=60 timeout=320 state=started
    with_items: ec2.instances

  - name: Create a volume and attach
    ec2_vol:
      volume_size: 20
      instance: "{{item.id}}"
      region: "{{region}}"
    with_items: ec2.instances
    register: ec2_volumes

  - name: Add tags to volumes
    ec2_tag:
      resource: "{{item.volume_id}}"
      region: "{{region}}"
      state: present
      tags:
        Name: "{{ lookup('env', 'USER') }}-{{ key_name }}"
    with_items: ec2_volumes.results

  - name: Create new ssh key-pair#generate a keylocally - this  will generate an error(ignored) if key exists... 
    shell: ssh-keygen -q -t rsa -N "" 
               -f {{ new_priv_key }}
    delegate_to: localhost
    ignore_errors: yes

  - name: copy private ssh keys #copy the key over to ec2 - this asks for confirmation - fix that
    local_action: "command scp -rp3  -o StrictHostKeyChecking=no   {{ new_priv_key }} ubuntu@{{ hostvars.localhost.ec2.instances[0].public_dns_name }}:./.ssh/id_rsa"
    sudo: no

  - name: copy public ssh keys #copy the key over to ec2 workers
    local_action: "command scp -rp3 -o StrictHostKeyChecking=no   {{ new_pub_key }} ubuntu@{{ item.public_dns_name }}:./.ssh/id_rsa.pub"
    with_items: ec2.instances
    sudo: no




- name: Configure basic key stuff on all nodes
  hosts: nodes    
  remote_user: ubuntu
  sudo: yes
  gather_facts: false
  roles:
    - basenode
    - postgres
- name: Configure coordinators nodes
  hosts: coordinators
  remote_user: ubuntu
  sudo: yes
  gather_facts: false
  roles:
    - myria
